{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13f3222e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 1 page is sucessful\n",
      "the 2 page is sucessful\n",
      "the 3 page is sucessful\n",
      "the 4 page is sucessful\n",
      "the 5 page is sucessful\n",
      "the 6 page is sucessful\n",
      "the 7 page is sucessful\n",
      "the 8 page is sucessful\n",
      "the 9 page is sucessful\n",
      "the 10 page is sucessful\n",
      "the 11 page is sucessful\n",
      "the 12 page is sucessful\n",
      "the 13 page is sucessful\n",
      "the 14 page is sucessful\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m a \u001b[38;5;241m=\u001b[39m get_data(page)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m#反爬虫策略2：每次爬取随机间隔3-10s\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m    100\u001b[0m count\u001b[38;5;241m=\u001b[39mcount\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(count)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m page is sucessful\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "    \n",
    "#伪装请求头\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60',\n",
    "    'Opera/8.0 (Windows NT 5.1; U; en)',\n",
    "    'Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.50',\n",
    "    'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; en) Opera 9.50',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0',\n",
    "    'Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "    'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 ',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36',\n",
    "    \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:30.0) Gecko/20100101 Firefox/30.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/537.75.14\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Win64; x64; Trident/6.0)\",\n",
    "    'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11',\n",
    "    'Opera/9.25 (Windows NT 5.1; U; en)',\n",
    "    'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)',\n",
    "    'Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Kubuntu)',\n",
    "    'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.12) Gecko/20070731 Ubuntu/dapper-security Firefox/1.5.0.12',\n",
    "    'Lynx/2.8.5rel.1 libwww-FM/2.14 SSL-MM/1.4.1 GNUTLS/1.2.9',\n",
    "    \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.7 (KHTML, like Gecko) Ubuntu/11.04 Chromium/16.0.912.77 Chrome/16.0.912.77 Safari/535.7\",\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:10.0) Gecko/20100101 Firefox/10.0 \"\n",
    "]\n",
    "\n",
    "def getHeaders():\n",
    "    user_agent = user_agents[random.randint(0, len(user_agents)-1)] \n",
    "    headers = {\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "\n",
    "\n",
    "#对一个URL发送请求，解析结果，获取所需数据\n",
    "def get_data(url):\n",
    "    response = requests.get(url, headers=getHeaders(), stream=True)\n",
    "    tree = etree.HTML(response.text)\n",
    "    li_list = tree.xpath('//div[@class=\"content w1150\"]/div[@class=\"content__article\"]/div[@class=\"content__list\"]/div')\n",
    "    for li in li_list:\n",
    "        try:\n",
    "            Nbhood = li.xpath('.//div[@class=\"content__list--item--main\"]/p[@class=\"content__list--item--title\"]/a/text()')[0].strip().split(' ')[0].split('·')[1]\n",
    "        except Exception:\n",
    "            Nbhood = ''\n",
    "        try:\n",
    "            LeaseMethod = li.xpath('.//div[@class=\"content__list--item--main\"]/p[@class=\"content__list--item--title\"]/a/text()')[0].strip().split(' ')[0].split('·')[0]\n",
    "        except Exception:\n",
    "            LeaseMethod = ''\n",
    "        try:\n",
    "            HouseOrientation = li.xpath('.//div[@class=\"content__list--item--main\"]/p[@class=\"content__list--item--title\"]/a/text()')[0].strip().split(' ')[2]\n",
    "        except Exception:\n",
    "            HouseOrientation = ''\n",
    "        try:\n",
    "            Rent = li.xpath('.//div[@class=\"content__list--item--main\"]/span[@class=\"content__list--item-price\"]/em/text()')[0]\n",
    "        except Exception:\n",
    "            Rent = ''\n",
    "        try:\n",
    "            District = li.xpath('.//div[@class=\"content__list--item--main\"]/p[@class=\"content__list--item--des\"]/a/text()')[0]\n",
    "        except Exception:\n",
    "            District = ''\n",
    "        try:\n",
    "            Location = li.xpath('.//div[@class=\"content__list--item--main\"]/p[@class=\"content__list--item--des\"]/a/text()')[1]\n",
    "        except Exception:\n",
    "            Location = ''\n",
    "        try:\n",
    "            Size = li.xpath('.//div[@class=\"content__list--item--main\"]/p[@class=\"content__list--item--des\"]/text()')[4].strip()\n",
    "        except Exception:\n",
    "            Size = ''\n",
    "        try:\n",
    "            HouseType = li.xpath('.//div[@class=\"content__list--item--main\"]/p[@class=\"content__list--item--des\"]/text()')[6].strip()\n",
    "        except Exception:\n",
    "            HouseType = ''\n",
    "        try:\n",
    "            releaseTime = li.xpath('.//div[@class=\"content__list--item--main\"]/p[@class=\"content__list--item--brand oneline\"]/span[@class=\"content__list--item--time oneline\"]/text()')[0]\n",
    "        except Exception:\n",
    "            releaseTime = ''\n",
    "        # Link 字段如果你不需要可以去掉\n",
    "        all_house_list.append((Nbhood, LeaseMethod, HouseOrientation, Rent, District, Location, Size, HouseType, releaseTime))\n",
    "    return all_house_list\n",
    "        \n",
    "    return all_house_list\n",
    "\n",
    "#循环爬取所需租房信息\n",
    "pages = ['https://sh.lianjia.com/ditiezufang/li143685063/pg{}rt200600000001l1l0ra1ra2ra0rp5rp6/'.format(x) for x in range(1,29)]\n",
    "all_house_list = []\n",
    "count = 0\n",
    "for page in pages:\n",
    "    a = get_data(page)\n",
    "    #反爬虫策略2：每次爬取随机间隔3-10s\n",
    "    time.sleep(random.randint(3,10))\n",
    "    count=count+1\n",
    "    print ('the '+str(count)+' page is sucessful')\n",
    "\n",
    "name = [\"街区\", \"租赁方式\", \"朝向\", \"每月租金\", \"行政区\",\"板块\",\"房屋面积\",\"格局\",\"发布时长\"]\n",
    "page_data = pd.DataFrame( columns= name,data=all_house_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66034d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 第 1 页爬取成功，获取 30 条\n",
      "✅ 第 2 页爬取成功，获取 30 条\n",
      "✅ 第 3 页爬取成功，获取 30 条\n",
      "✅ 第 4 页爬取成功，获取 30 条\n",
      "✅ 第 5 页爬取成功，获取 30 条\n",
      "✅ 第 6 页爬取成功，获取 30 条\n",
      "✅ 第 7 页爬取成功，获取 30 条\n",
      "✅ 第 8 页爬取成功，获取 30 条\n",
      "✅ 第 9 页爬取成功，获取 30 条\n",
      "✅ 第 10 页爬取成功，获取 30 条\n",
      "✅ 第 11 页爬取成功，获取 30 条\n",
      "✅ 第 12 页爬取成功，获取 30 条\n",
      "✅ 第 13 页爬取成功，获取 30 条\n",
      "✅ 第 14 页爬取成功，获取 30 条\n",
      "✅ 第 15 页爬取成功，获取 30 条\n",
      "✅ 第 16 页爬取成功，获取 30 条\n",
      "✅ 第 17 页爬取成功，获取 30 条\n",
      "[请求失败] HTTPSConnectionPool(host='sh.lianjia.com', port=443): Max retries exceeded with url: /zufang/pg18/ (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n",
      "⚠️ 第 18 页返回为空，可能被拦截\n",
      "✅ 第 19 页爬取成功，获取 30 条\n",
      "✅ 第 20 页爬取成功，获取 30 条\n",
      "✅ 第 21 页爬取成功，获取 30 条\n",
      "✅ 第 22 页爬取成功，获取 30 条\n",
      "[请求失败] HTTPSConnectionPool(host='sh.lianjia.com', port=443): Max retries exceeded with url: /zufang/pg23/ (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n",
      "⚠️ 第 23 页返回为空，可能被拦截\n",
      "✅ 第 24 页爬取成功，获取 30 条\n",
      "✅ 第 25 页爬取成功，获取 30 条\n",
      "✅ 第 26 页爬取成功，获取 30 条\n",
      "✅ 第 27 页爬取成功，获取 30 条\n",
      "[请求失败] HTTPSConnectionPool(host='sh.lianjia.com', port=443): Max retries exceeded with url: /zufang/pg28/ (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n",
      "⚠️ 第 28 页返回为空，可能被拦截\n",
      "✅ 第 29 页爬取成功，获取 30 条\n",
      "✅ 第 30 页爬取成功，获取 30 条\n",
      "✅ 第 31 页爬取成功，获取 30 条\n",
      "✅ 第 32 页爬取成功，获取 30 条\n",
      "✅ 第 33 页爬取成功，获取 30 条\n",
      "✅ 第 34 页爬取成功，获取 30 条\n",
      "✅ 第 35 页爬取成功，获取 30 条\n",
      "✅ 第 36 页爬取成功，获取 30 条\n",
      "✅ 第 37 页爬取成功，获取 30 条\n",
      "✅ 第 38 页爬取成功，获取 30 条\n",
      "✅ 第 39 页爬取成功，获取 30 条\n",
      "[请求失败] HTTPSConnectionPool(host='sh.lianjia.com', port=443): Max retries exceeded with url: /zufang/pg40/ (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n",
      "⚠️ 第 40 页返回为空，可能被拦截\n",
      "[请求失败] HTTPSConnectionPool(host='sh.lianjia.com', port=443): Max retries exceeded with url: /zufang/pg41/ (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n",
      "⚠️ 第 41 页返回为空，可能被拦截\n",
      "✅ 第 42 页爬取成功，获取 30 条\n",
      "✅ 第 43 页爬取成功，获取 30 条\n",
      "✅ 第 44 页爬取成功，获取 30 条\n",
      "✅ 第 45 页爬取成功，获取 30 条\n",
      "✅ 第 46 页爬取成功，获取 30 条\n",
      "[请求失败] HTTPSConnectionPool(host='sh.lianjia.com', port=443): Max retries exceeded with url: /zufang/pg47/ (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n",
      "⚠️ 第 47 页返回为空，可能被拦截\n",
      "✅ 第 48 页爬取成功，获取 30 条\n",
      "✅ 第 49 页爬取成功，获取 30 条\n",
      "✅ 第 50 页爬取成功，获取 30 条\n",
      "✅ 第 51 页爬取成功，获取 30 条\n",
      "✅ 第 52 页爬取成功，获取 30 条\n",
      "✅ 第 53 页爬取成功，获取 30 条\n",
      "✅ 第 54 页爬取成功，获取 30 条\n",
      "✅ 第 55 页爬取成功，获取 30 条\n",
      "✅ 第 56 页爬取成功，获取 30 条\n",
      "✅ 第 57 页爬取成功，获取 30 条\n",
      "✅ 第 58 页爬取成功，获取 30 条\n",
      "✅ 第 59 页爬取成功，获取 30 条\n",
      "✅ 第 60 页爬取成功，获取 30 条\n",
      "[请求失败] HTTPSConnectionPool(host='sh.lianjia.com', port=443): Max retries exceeded with url: /zufang/pg61/ (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n",
      "⚠️ 第 61 页返回为空，可能被拦截\n",
      "[请求失败] HTTPSConnectionPool(host='sh.lianjia.com', port=443): Max retries exceeded with url: /zufang/pg62/ (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n",
      "⚠️ 第 62 页返回为空，可能被拦截\n",
      "✅ 第 63 页爬取成功，获取 30 条\n",
      "✅ 第 64 页爬取成功，获取 30 条\n",
      "✅ 第 65 页爬取成功，获取 30 条\n",
      "[请求失败] HTTPSConnectionPool(host='sh.lianjia.com', port=443): Max retries exceeded with url: /zufang/pg66/ (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n",
      "⚠️ 第 66 页返回为空，可能被拦截\n",
      "✅ 第 67 页爬取成功，获取 30 条\n",
      "[请求失败] HTTPSConnectionPool(host='sh.lianjia.com', port=443): Max retries exceeded with url: /zufang/pg68/ (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n",
      "⚠️ 第 68 页返回为空，可能被拦截\n",
      "✅ 第 69 页爬取成功，获取 30 条\n",
      "✅ 第 70 页爬取成功，获取 30 条\n",
      "[请求失败] HTTPSConnectionPool(host='sh.lianjia.com', port=443): Max retries exceeded with url: /zufang/pg71/ (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n",
      "⚠️ 第 71 页返回为空，可能被拦截\n",
      "✅ 第 72 页爬取成功，获取 30 条\n",
      "✅ 第 73 页爬取成功，获取 30 条\n",
      "✅ 第 74 页爬取成功，获取 30 条\n",
      "✅ 第 75 页爬取成功，获取 30 条\n",
      "✅ 第 76 页爬取成功，获取 30 条\n",
      "✅ 第 77 页爬取成功，获取 30 条\n",
      "✅ 第 78 页爬取成功，获取 30 条\n",
      "✅ 第 79 页爬取成功，获取 30 条\n",
      "✅ 第 80 页爬取成功，获取 30 条\n",
      "✅ 第 81 页爬取成功，获取 30 条\n",
      "✅ 第 82 页爬取成功，获取 30 条\n",
      "✅ 第 83 页爬取成功，获取 30 条\n",
      "✅ 第 84 页爬取成功，获取 30 条\n",
      "✅ 第 85 页爬取成功，获取 30 条\n",
      "✅ 第 86 页爬取成功，获取 30 条\n",
      "[请求失败] HTTPSConnectionPool(host='sh.lianjia.com', port=443): Max retries exceeded with url: /zufang/pg87/ (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n",
      "⚠️ 第 87 页返回为空，可能被拦截\n",
      "✅ 第 88 页爬取成功，获取 30 条\n",
      "[请求失败] HTTPSConnectionPool(host='sh.lianjia.com', port=443): Max retries exceeded with url: /zufang/pg89/ (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n",
      "⚠️ 第 89 页返回为空，可能被拦截\n",
      "✅ 第 90 页爬取成功，获取 30 条\n",
      "✅ 第 91 页爬取成功，获取 30 条\n",
      "✅ 第 92 页爬取成功，获取 30 条\n",
      "✅ 第 93 页爬取成功，获取 30 条\n",
      "✅ 第 94 页爬取成功，获取 30 条\n",
      "✅ 第 95 页爬取成功，获取 30 条\n",
      "[请求失败] HTTPSConnectionPool(host='sh.lianjia.com', port=443): Max retries exceeded with url: /zufang/pg96/ (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n",
      "⚠️ 第 96 页返回为空，可能被拦截\n",
      "✅ 第 97 页爬取成功，获取 30 条\n",
      "✅ 第 98 页爬取成功，获取 30 条\n",
      "✅ 第 99 页爬取成功，获取 30 条\n",
      "✅ 第 100 页爬取成功，获取 30 条\n",
      "📁 数据保存完成：链家租房数据.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# 常见 User-Agent 随机池\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 Chrome/39.0.2171.95 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/83.0.4103.97 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 Version/13.1 Safari/605.1.15'\n",
    "]\n",
    "\n",
    "# ⚠️ 替换为你抓包拿到的完整 Cookie 字符串\n",
    "cookie_str = '''\n",
    "lianjia_uuid=94d7f3e2-9df4-4d2f-8fdd-f168554a2c15; Hm_lvt_46bf127ac9b856df503ec2dbf942b67e=1749713732; _jzqa=1.410847021484261760.1749713732.1749713732.1749713732.1; _jzqx=1.1749713732.1749713732.1.jzqsr=hip%2Elianjia%2Ecom|jzqct=/.-; _jzqckmp=1; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%221976310c3ed146e-0559479a0beded-26011f51-1327104-1976310c3ee11f4%22%2C%22%24device_id%22%3A%221976310c3ed146e-0559479a0beded-26011f51-1327104-1976310c3ee11f4%22%2C%22props%22%3A%7B%7D%7D; _ga=GA1.2.1593411520.1749713743; _gid=GA1.2.1823501581.1749713743; _ga_4JBJY7Y7MX=GS2.2.s1749713743$o1$g0$t1749713743$j60$l0$h0; lianjia_ssid=05d120a9-0862-4e0e-ad4e-b51137a97590; hip=f0YdttuzkJhb5EFDMYxVAP_IBiJLQZgLkhhx-WF3AwQWh0oOXDORPOA22zd_LXqk3nZp0I5Qmsgkqt9c5zxtfrj3DdVLNkRhSLMBQPSKt7OUl6R_ux5yw5MJKzFseRdvKzy5EGU5Nzhw6Ihyn6rD0hA0X24iIf6h0ANP_Bi2Bywd_lChZiUfL85m9w%3D%3D; select_city=310000; GUARANTEE_POPUP_SHOW=true; GUARANTEE_BANNER_SHOW=true\n",
    "'''.strip()\n",
    "\n",
    "def getHeaders():\n",
    "    user_agent = random.choice(user_agents)\n",
    "    headers = {\n",
    "        'User-Agent': user_agent,\n",
    "        'Cookie': cookie_str,\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "def get_data(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=getHeaders(), timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"[请求失败] {e}\")\n",
    "        return []\n",
    "\n",
    "    tree = etree.HTML(response.text)\n",
    "    li_list = tree.xpath('//div[@class=\"content__list\"]/div[@class=\"content__list--item\"]')\n",
    "    result = []\n",
    "\n",
    "    for li in li_list:\n",
    "        try:\n",
    "            Nbhood = li.xpath('.//p[@class=\"content__list--item--title twoline\"]/a/text()')[0].strip().split(' ')[0].split('·')[1]\n",
    "        except:\n",
    "            Nbhood = ''\n",
    "        try:\n",
    "            LeaseMethod = li.xpath('.//p[@class=\"content__list--item--title twoline\"]/a/text()')[0].strip().split(' ')[0].split('·')[0]\n",
    "        except:\n",
    "            LeaseMethod = ''\n",
    "        try:\n",
    "            HouseOrientation = li.xpath('.//p[@class=\"content__list--item--title twoline\"]/a/text()')[0].strip().split(' ')[2]\n",
    "        except:\n",
    "            HouseOrientation = ''\n",
    "        try:\n",
    "            Rent = li.xpath('.//span[@class=\"content__list--item-price\"]/em/text()')[0]\n",
    "        except:\n",
    "            Rent = ''\n",
    "        try:\n",
    "            District = li.xpath('.//p[@class=\"content__list--item--des\"]/a[1]/text()')[0]\n",
    "        except:\n",
    "            District = ''\n",
    "        try:\n",
    "            Location = li.xpath('.//p[@class=\"content__list--item--des\"]/a[2]/text()')[0]\n",
    "        except:\n",
    "            Location = ''\n",
    "        try:\n",
    "            Size = li.xpath('.//p[@class=\"content__list--item--des\"]/text()')[4].strip()\n",
    "        except:\n",
    "            Size = ''\n",
    "        try:\n",
    "            HouseType = li.xpath('.//p[@class=\"content__list--item--des\"]/text()')[6].strip()\n",
    "        except:\n",
    "            HouseType = ''\n",
    "        try:\n",
    "            releaseTime = li.xpath('.//p[@class=\"content__list--item--brand oneline\"]/span/text()')[0]\n",
    "        except:\n",
    "            releaseTime = ''\n",
    "\n",
    "        result.append((Nbhood, LeaseMethod, HouseOrientation, Rent, District, Location, Size, HouseType, releaseTime))\n",
    "\n",
    "    return result\n",
    "\n",
    "# 示例：爬取前5页数据\n",
    "pages = ['https://sh.lianjia.com/zufang/pg{}/'.format(i) for i in range(1, 101)]\n",
    "all_house_list = []\n",
    "\n",
    "for i, page in enumerate(pages, start=1):\n",
    "    data = get_data(page)\n",
    "    if data:\n",
    "        all_house_list.extend(data)\n",
    "        print(f'✅ 第 {i} 页爬取成功，获取 {len(data)} 条')\n",
    "    else:\n",
    "        print(f'⚠️ 第 {i} 页返回为空，可能被拦截')\n",
    "    time.sleep(random.randint(3, 7))\n",
    "\n",
    "# 保存数据\n",
    "columns = [\"街区\", \"租赁方式\", \"朝向\", \"每月租金\", \"行政区\", \"板块\", \"房屋面积\", \"格局\", \"发布时长\"]\n",
    "df = pd.DataFrame(all_house_list, columns=columns)\n",
    "df.to_csv('链家租房数据.csv', index=False, encoding='utf_8_sig')\n",
    "print(\"📁 数据保存完成：链家租房数据.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
