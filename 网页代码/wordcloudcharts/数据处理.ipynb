{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e59dec77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已过滤停用词并增强词频差异，保存至 word_freq.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "\n",
    "# 停用词列表（可扩展）\n",
    "stop_words = set([\n",
    "    '的', '了', '是', '我', '也', '和', '就', '都', '很', '在', '有',\n",
    "    '一个', '这个', '那个', '我们', '你们', '他们', '但', '被', '等',\n",
    "    '上', '下', '中', '说', '着', '自己', '不会', '可以', '因为', '还',\n",
    "    '让', '去', '要', '呢', '啊', '吧', '呀', '哦', '嘛', '啦','超话','人一','搜索','微博','话题','评论','点赞','转发','关注','粉丝',\n",
    "    '回复', '发布', '内容', '信息', '平台', '用户', '账号', '数据', '链接', '查看','什么', '怎么', '为什么', '哪里', '谁', '多少', '如何', '怎样',\n",
    "    '时间', '问题', '答案', '回答', '文章', '博文', '微博搜索', '关键词', '采集', '分析','上海','郑州','杭州','视频','其他','或者','直接','浙江','的话',\n",
    "    '南京','广州','深圳','北京','天津','重庆','武汉','西安','成都','长沙','青岛','苏州','厦门','济南','合肥','无锡','所以','齐齐哈尔','淮安','身边','可能','就是','以及','一些',\n",
    "    '一天','一定','一下','一般','已经','一直','一些','这种','招聘','而且','尽量','不用','不要','同好','如果','没有'\n",
    "])\n",
    "\n",
    "\n",
    "# 读取 Excel\n",
    "df = pd.read_excel(r\"C:\\Users\\ThinkPad\\Desktop\\信息可视化\\期末作业\\微博搜索关键词采集.xlsx\")\n",
    "texts = df['博文内容'].dropna().tolist()\n",
    "\n",
    "# 清洗文本\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5]', '', text)\n",
    "    return text\n",
    "\n",
    "cleaned = [clean_text(text) for text in texts]\n",
    "\n",
    "# 分词 + 去停用词\n",
    "all_words = []\n",
    "for text in cleaned:\n",
    "    words = jieba.cut(text)\n",
    "    all_words.extend([w for w in words if len(w) > 1 and w not in stop_words])\n",
    "\n",
    "# 统计词频\n",
    "counter = Counter(all_words)\n",
    "most_common = counter.most_common(100)\n",
    "\n",
    "# 手动扩大频率差异（幂次增强）\n",
    "scaled_data = []\n",
    "for word, freq in most_common:\n",
    "    scaled_data.append({\n",
    "        \"name\": word,\n",
    "        \"value\": int(freq ** 1.5)  # 增强视觉差异（可调 1.5 ~ 2.0）\n",
    "    })\n",
    "\n",
    "# 保存为 JSON\n",
    "with open(\"weibo_word_freq.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(scaled_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"已过滤停用词并增强词频差异，保存至 word_freq.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de51ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "file_path = r'C:\\Users\\ThinkPad\\Desktop\\信息可视化\\期末作业\\知乎关键词搜索问答采集.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "texts = df['问题标题'].astype(str).tolist() + df['回答内容'].astype(str).tolist()\n",
    "\n",
    "# 清洗文本\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5]', '', text)\n",
    "    return text\n",
    "cleaned_texts = [clean_text(text) for text in texts if isinstance(text, str) and text.strip()]\n",
    "texts = cleaned_texts\n",
    "# 分词\n",
    "words = []\n",
    "for text in texts:\n",
    "    words.extend([w for w in jieba.cut(text) if len(w) > 1])  # 去除单字词\n",
    "\n",
    "# 统计词频\n",
    "counter = Counter(words)\n",
    "\n",
    "# 过滤掉常见无意义词\n",
    "stop_words = set([\n",
    "    '的', '了', '是', '我', '也', '和', '就', '都', '很', '在', '有',\n",
    "    '一个', '这个', '那个', '我们', '你们', '他们', '但', '被', '等',\n",
    "    '上', '下', '中', '说', '着', '自己', '不会', '可以', '因为', '还',\n",
    "    '让', '去', '要', '呢', '啊', '吧', '呀', '哦', '嘛', '啦','超话','人一','搜索','微博','话题','评论','点赞','转发','关注','粉丝',\n",
    "    '回复', '发布', '内容', '信息', '平台', '用户', '账号', '数据', '链接', '查看','什么', '怎么', '为什么', '哪里', '谁', '多少', '如何', '怎样',\n",
    "    '时间', '问题', '答案', '回答', '文章', '博文', '微博搜索', '关键词', '采集', '分析','上海','郑州','杭州','视频','其他','或者','直接','浙江','的话',\n",
    "    '南京','广州','深圳','北京','天津','重庆','武汉','西安','成都','长沙','青岛','苏州','厦门','济南','合肥','无锡','所以','齐齐哈尔','淮安','身边','可能','就是','以及','一些',\n",
    "    '一天','一定','一下','一般','已经','一直','一些','这种','招聘','而且','尽量','不用','不要','同好','如果','没有'\n",
    "])\n",
    "\n",
    "filtered = {word: freq for word, freq in counter.items() if word not in stop_words}\n",
    "\n",
    "# 转成词云需要的格式\n",
    "word_freq_list = [{\"name\": word, \"value\": freq} for word, freq in filtered.items()]\n",
    "\n",
    "# 保存为json文件，供echarts调用\n",
    "import json\n",
    "with open('zhihu_word_freq.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(word_freq_list, f, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c8cfe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_word_freq(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def merge_word_freq(list1, list2):\n",
    "    merged = defaultdict(int)\n",
    "    for item in list1:\n",
    "        merged[item['name']] += item['value']\n",
    "    for item in list2:\n",
    "        merged[item['name']] += item['value']\n",
    "    return [{\"name\": k, \"value\": v} for k, v in merged.items()]\n",
    "\n",
    "weibo_data = load_word_freq('weibo_word_freq.json')\n",
    "zhihu_data = load_word_freq('zhihu_word_freq.json')\n",
    "\n",
    "merged_data = merge_word_freq(weibo_data, zhihu_data)\n",
    "\n",
    "with open('merged_word_freq.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(merged_data, f, ensure_ascii=False, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
